# ğŸ—ï¸ INTER-SIGHT - FINAL ARCHITECTURE & IMPLEMENTATION PLAN

## ğŸ¯ PROJECT LOCKED - FINAL SPECIFICATIONS

```
PROJECT NAME:     Inter-Sight
HACKATHON:        Hack-Nation Global AI Hackathon (Nov 8-9, 2025)
CHALLENGE:        SAP SkillSense - AI for Talent Identification
TIME BUDGET:      24 hours
LANGUAGE:         English (all code + docs)

FINAL DECISIONS (LOCKED):
âœ… LLM Strategy:       Agnostic API wrapper (Mistral free default, configurable)
âœ… Data Strategy:      Real diverse CVs + Realistic company profiles
âœ… UI/UX Strategy:     Visually polished, impactful Streamlit design
âœ… Feedback:           Hybrid (template base + LLM personalization)
âœ… Red Flags:          Comprehensive detection + contextualization
âœ… CV Processing:      Batch (multiple CVs, ranked leaderboard)
âœ… UX Pattern:         Tinder-like cards + Detail views (beautiful)
âœ… Deployment:         Streamlit local + Cloud ready
```

---

## ğŸ›ï¸ CORE ARCHITECTURE

### Layer 1: LLM ABSTRACTION (Agnostic)

```python
# llm_provider.py - Abstract interface
class LLMProvider(ABC):
    """Abstract base for any LLM provider"""
    
    @abstractmethod
    def generate_text(self, prompt: str, max_tokens: int = 500) -> str:
        """Generate text from prompt"""
        pass
    
    @abstractmethod
    def extract_json(self, prompt: str, schema: dict) -> dict:
        """Generate structured JSON output"""
        pass
    
    @abstractmethod
    def batch_process(self, prompts: list) -> list:
        """Process multiple prompts efficiently"""
        pass

# Implementations:
class MistralProvider(LLMProvider):
    """Mistral API (free tier available)"""
    
class ClaudeProvider(LLMProvider):
    """Claude API (Anthropic)"""
    
class OpenAIProvider(LLMProvider):
    """GPT-4/3.5 (OpenAI)"""
    
class LocalLlamaProvider(LLMProvider):
    """Local Llama via Ollama (no API key needed)"""
```

### Layer 2: MODULAR PROCESSING PIPELINE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INTER-SIGHT PROCESSING PIPELINE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

INPUT STAGE:
â”œâ”€ CompanyProfiler
â”‚  â”œâ”€ Collect: mission, vision, values, soft_skills_weights
â”‚  â””â”€ LLM: Generate culture vector + skill taxonomy
â”‚
â””â”€ CVBatchLoader
   â”œâ”€ Accept: PDF, TXT, JSON files (multiple)
   â””â”€ Queue for processing

PROCESSING STAGE (per CV):
â”œâ”€ CVParser
â”‚  â”œâ”€ Extract: structure (name, education, experience)
â”‚  â””â”€ Quality scoring (completeness, quantification)
â”‚
â”œâ”€ SkillExtractor
â”‚  â”œâ”€ Heuristic: Technical skills (keywords)
â”‚  â”œâ”€ LLM: Soft skills inference (semantic)
â”‚  â””â”€ Confidence scoring
â”‚
â”œâ”€ CultureAnalyzer
â”‚  â”œâ”€ Parse: Implicit values from CV text
â”‚  â””â”€ Map: To company values (semantic matching)
â”‚
â””â”€ RedFlagDetector
   â”œâ”€ Detect: Job hopping, gaps, overqualification
   â”œâ”€ LLM: Context analysis (why did this happen?)
   â””â”€ Severity + Interview questions

MATCHING STAGE:
â”œâ”€ MultiLayerMatcher
â”‚  â”œâ”€ Technical match (35%): skill confidence
â”‚  â”œâ”€ Soft skills match (30%): semantic to values
â”‚  â”œâ”€ Culture fit (20%): values alignment
â”‚  â””â”€ CV quality (15%): structure + metrics
â”‚
â””â”€ Score calculation â†’ Rank candidates

FEEDBACK GENERATION:
â”œâ”€ TemplateFeedback
â”‚  â”œâ”€ Base: Why matched/didn't + gaps
â”‚  â””â”€ Format: Clear, structured, scannable
â”‚
â””â”€ LLMFeedback (optional enhancement)
   â”œâ”€ If score < 50: "Why you didn't match" (personalized)
   â”œâ”€ If score > 75: "Your unique strengths"
   â””â”€ Always: "Top 3 improvements" (actionable)

OUTPUT STAGE:
â”œâ”€ RankedCandidateList
â”‚  â”œâ”€ Sort by match score
â”‚  â””â”€ Include: Basic metadata + match %
â”‚
â”œâ”€ DetailedFeedback (per candidate)
â”‚  â”œâ”€ Match breakdown
â”‚  â”œâ”€ Red flags with context
â”‚  â”œâ”€ Improvement roadmap
â”‚  â””â”€ Interview talking points
â”‚
â””â”€ UI Rendering (Streamlit)
   â”œâ”€ Card view (Tinder-like)
   â””â”€ Detail view (full feedback)
```

---

## ğŸ“ PROJECT STRUCTURE

```
inter-sight/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_provider.py           â† LLM abstraction (KEY)
â”‚   â”œâ”€â”€ config.py                 â† Config loader (.env)
â”‚   â””â”€â”€ constants.py              â† Constants, schemas
â”‚
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ company_profiler.py       â† Company profile builder
â”‚   â”œâ”€â”€ cv_loader.py              â† Batch CV loading
â”‚   â”œâ”€â”€ cv_parser.py              â† Parse CV structure
â”‚   â”œâ”€â”€ skill_extractor.py        â† Extract skills (hybrid)
â”‚   â”œâ”€â”€ culture_analyzer.py       â† Culture matching
â”‚   â”œâ”€â”€ red_flag_detector.py      â† Comprehensive flags
â”‚   â”œâ”€â”€ matcher.py                â† Multi-layer matching
â”‚   â””â”€â”€ feedback_generator.py     â† Feedback generation
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ streamlit_app.py          â† Main Streamlit app
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ card_view.py          â† Tinder card UI
â”‚   â”‚   â”œâ”€â”€ detail_view.py        â† Feedback detail
â”‚   â”‚   â”œâ”€â”€ company_form.py       â† Company setup
â”‚   â”‚   â””â”€â”€ cv_upload.py          â† CV upload widget
â”‚   â””â”€â”€ styles.py                 â† CSS/styling
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_cvs/               â† Diverse sample CVs
â”‚   â”‚   â”œâ”€â”€ alice_tech_lead.json
â”‚   â”‚   â”œâ”€â”€ bob_career_changer.json
â”‚   â”‚   â”œâ”€â”€ carol_researcher.json
â”‚   â”‚   â”œâ”€â”€ david_founder.json
â”‚   â”‚   â””â”€â”€ eva_data_scientist.json
â”‚   â”‚
â”‚   â””â”€â”€ sample_companies/         â† Sample company profiles
â”‚       â”œâ”€â”€ techstartup_xyz.json
â”‚       â””â”€â”€ sapcorp.json
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py                 â† Logging setup
â”‚   â”œâ”€â”€ validators.py             â† Input validation
â”‚   â””â”€â”€ formatters.py             â† Output formatting
â”‚
â”œâ”€â”€ .env.example                  â† Config template
â”œâ”€â”€ requirements.txt              â† Dependencies
â”œâ”€â”€ README.md                     â† Documentation
â””â”€â”€ main.py                       â† Entry point (optional)
```

---

## ğŸ”§ DEPENDENCIES & SETUP

### requirements.txt
```
# Core
streamlit==1.28.0
pandas==2.1.0
numpy==1.24.0

# LLM Abstraction
anthropic==0.7.0              # Claude
openai==1.3.0                 # GPT
mistralai==0.0.11             # Mistral (FREE tier available)

# Data Processing
pydantic==2.4.0               # Config + validation
python-dotenv==1.0.0          # .env support

# File Handling
pdfplumber==0.9.0             # PDF parsing
python-magic==0.4.27          # File type detection

# UI/UX
plotly==5.17.0                # Charts
streamlit-extras==0.3.0       # Extra components

# Utilities
python-dateutil==2.8.2
pytz==2023.3
```

### .env Configuration

```env
# LLM Provider (choose one: mistral, claude, openai, llama)
LLM_PROVIDER=mistral

# Mistral (Free tier: https://console.mistral.ai/)
MISTRAL_API_KEY=your_mistral_key_here
MISTRAL_MODEL=mistral-large

# Claude (optional fallback)
# CLAUDE_API_KEY=your_claude_key_here

# OpenAI (optional fallback)
# OPENAI_API_KEY=your_openai_key_here

# App Configuration
APP_DEBUG=false
MAX_BATCH_CVS=20
FEEDBACK_MODE=hybrid  # hybrid, template, llm
```

---

## ğŸ¯ CORE COMPONENTS (Detailed Design)

### 1. LLM Provider (llm_provider.py)

```python
from abc import ABC, abstractmethod
from typing import Optional, Dict, List
from pydantic import BaseModel

class LLMConfig(BaseModel):
    provider: str              # mistral, claude, openai, llama
    api_key: str
    model: str
    temperature: float = 0.7
    max_tokens: int = 1000

class LLMProvider(ABC):
    """Abstract base class for LLM providers"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
    
    @abstractmethod
    def generate_text(self, prompt: str, **kwargs) -> str:
        """Generate text completion"""
        pass
    
    @abstractmethod
    def extract_json(self, prompt: str, schema: Dict) -> Dict:
        """Generate JSON response (structured output)"""
        pass
    
    @abstractmethod
    def batch_process(self, prompts: List[str]) -> List[str]:
        """Process multiple prompts"""
        pass

class MistralProvider(LLMProvider):
    """Mistral AI implementation (FREE tier available)"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        from mistralai.client import MistralClient
        self.client = MistralClient(api_key=config.api_key)
    
    def generate_text(self, prompt: str, **kwargs) -> str:
        from mistralai.models.chat_message import ChatMessage
        
        messages = [ChatMessage(role="user", content=prompt)]
        response = self.client.chat(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        return response.choices[0].message.content
    
    def extract_json(self, prompt: str, schema: Dict) -> Dict:
        """Generate JSON using Mistral"""
        prompt_with_schema = f"""{prompt}

Return ONLY valid JSON matching this structure:
{json.dumps(schema, indent=2)}

NO additional text. NO markdown. ONLY JSON."""
        
        response = self.generate_text(prompt_with_schema)
        return json.loads(response)
    
    def batch_process(self, prompts: List[str]) -> List[str]:
        """Process multiple prompts (sequential for free tier)"""
        return [self.generate_text(p) for p in prompts]

# Factory
def get_llm_provider(config: LLMConfig) -> LLMProvider:
    providers = {
        "mistral": MistralProvider,
        "claude": ClaudeProvider,
        "openai": OpenAIProvider,
        "llama": LocalLlamaProvider,
    }
    
    provider_class = providers.get(config.provider)
    if not provider_class:
        raise ValueError(f"Unknown LLM provider: {config.provider}")
    
    return provider_class(config)
```

### 2. Comprehensive Red Flag Detector

```python
class RedFlagDetector:
    """Detect and contextualize red flags"""
    
    def detect(self, cv_data: Dict, llm: LLMProvider) -> List[Dict]:
        """Detect all red flags with context"""
        
        flags = []
        
        # 1. Job Stability Analysis
        flags.extend(self._check_job_stability(cv_data, llm))
        
        # 2. Experience Gaps
        flags.extend(self._check_gaps(cv_data, llm))
        
        # 3. Skill Progression
        flags.extend(self._check_skill_progression(cv_data, llm))
        
        # 4. CV Quality
        flags.extend(self._check_cv_quality(cv_data))
        
        # 5. Overqualification/Underqualification
        flags.extend(self._check_qualification_fit(cv_data, llm))
        
        return flags
    
    def _check_job_stability(self, cv_data: Dict, llm: LLMProvider) -> List[Dict]:
        """Analyze job hopping with LLM context"""
        
        experience = cv_data.get("experience", [])
        if len(experience) < 2:
            return []
        
        jobs_per_year = len(experience) / max(1, self._estimate_years(experience))
        
        if jobs_per_year > 1.5:
            # Get LLM context
            context_prompt = f"""
            This candidate had {len(experience)} jobs in ~{self._estimate_years(experience)} years.
            
            Possible explanations:
            1. Sector volatility (startup, consultancy)
            2. Economic factors (layoffs)
            3. Candidate choice (active searching)
            4. Interpersonal issues
            
            Analyze and provide context. What would you ask in an interview?
            """
            
            context = llm.generate_text(context_prompt)
            
            return [{
                "name": "ğŸ”´ Job Hopping",
                "severity": "critical",
                "description": f"{jobs_per_year:.1f} jobs/year detected",
                "context": context,
                "interview_question": "Walk me through your career transitions. What drove each change?",
                "actionable": True
            }]
        
        return []
    
    def _check_cv_quality(self, cv_data: Dict) -> List[Dict]:
        """Detect CV structure issues"""
        
        flags = []
        quality = cv_data.get("cv_quality", {})
        
        if quality.get("overall", 0) < 0.6:
            issues = quality.get("issues", [])
            flags.append({
                "name": "âŒ CV Quality Issues",
                "severity": "major",
                "description": f"CV lacks structure/metrics: {', '.join(issues)}",
                "context": "CVs without quantified achievements are harder to evaluate",
                "improvement": "Add metrics: Instead of 'improved performance', say 'improved by 40%'",
                "actionable": True
            })
        
        return flags
    
    # ... more detection methods
```

### 3. Hybrid Feedback Generator

```python
class FeedbackGenerator:
    """Generate hybrid feedback (template + LLM)"""
    
    def __init__(self, llm: LLMProvider, mode: str = "hybrid"):
        self.llm = llm
        self.mode = mode  # hybrid, template, llm
    
    def generate(self, 
                 company: Dict, 
                 candidate: Dict, 
                 match_score: Dict, 
                 red_flags: List[Dict]) -> Dict:
        """Generate comprehensive feedback"""
        
        feedback = {
            "summary": self._summary(match_score),
            "breakdown": self._breakdown(match_score),
            "why_matched": self._why_matched(match_score, candidate),
            "why_not_matched": self._why_not_matched(match_score, candidate),
        }
        
        # Add red flags analysis
        if red_flags:
            feedback["red_flags"] = self._analyze_red_flags(red_flags)
        
        # Add improvements
        if self.mode in ["hybrid", "llm"]:
            feedback["improvements"] = self._llm_improvements(
                company, candidate, match_score, red_flags
            )
        else:
            feedback["improvements"] = self._template_improvements(match_score, red_flags)
        
        # Add interview prep
        feedback["interview_talking_points"] = self._interview_points(
            candidate, company, red_flags
        )
        
        return feedback
    
    def _llm_improvements(self, company: Dict, candidate: Dict, 
                         match_score: Dict, red_flags: List[Dict]) -> List[Dict]:
        """Use LLM for personalized improvement suggestions"""
        
        prompt = f"""
        Candidate didn't match perfectly (score: {match_score['overall']:.0f}%).
        
        Gaps:
        - Technical: {match_score.get('technical', 0):.0f}%
        - Soft skills: {match_score.get('soft_skills', 0):.0f}%
        - Culture: {match_score.get('culture_fit', 0):.0f}%
        
        Red flags: {len(red_flags)} detected
        
        Generate 3 SPECIFIC, ACTIONABLE improvements this candidate should focus on.
        Order by priority. Make it motivational but honest.
        """
        
        response = self.llm.generate_text(prompt)
        
        # Parse response into structured improvements
        return self._parse_improvements(response)
    
    def _template_improvements(self, match_score: Dict, 
                              red_flags: List[Dict]) -> List[Dict]:
        """Template-based improvements (fast fallback)"""
        
        improvements = []
        
        if match_score.get("soft_skills", 0) < 60:
            improvements.append({
                "priority": 1,
                "area": "Soft Skills Development",
                "specific": "Leadership/Communication not demonstrated",
                "action": "Highlight mentoring, speaking, or leadership examples",
                "timeline": "3-6 months"
            })
        
        # ... more template-based improvements
        
        return improvements
```

---

## ğŸ¨ UI/UX DESIGN (Streamlit)

### Main App Structure

```python
# streamlit_app.py
import streamlit as st
from streamlit_extras.metric_row import metric_row
from components import card_view, detail_view, company_form, cv_upload

st.set_page_config(
    page_title="Inter-Sight",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS for beautiful design
st.markdown("""
<style>
    [data-testid="stMetricValue"] {
        font-size: 2.5rem;
    }
    .candidate-card {
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# Tabs
tab1, tab2, tab3 = st.tabs(["ğŸ¢ Company Setup", "ğŸ“¤ Upload CVs", "ğŸ¯ Results"])

with tab1:
    st.header("Company Profile")
    company_data = company_form.render()

with tab2:
    st.header("Upload Candidates")
    if company_data:
        cvs = cv_upload.render()

with tab3:
    st.header("Results & Insights")
    if company_data and cvs:
        # Process and display
        results = process_candidates(company_data, cvs)
        display_results(results)
```

### Beautiful Card Component

```python
# components/card_view.py
def render_card(candidate: Dict, match_score: Dict, index: int):
    """Render beautiful Tinder-like card"""
    
    # Color based on match
    color = get_match_color(match_score["overall"])
    emoji = get_match_emoji(match_score["overall"])
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        st.metric("Match", f"{match_score['overall']:.0f}%", 
                 delta=f"{emoji}")
    
    with col2:
        st.markdown(f"""
        ### {candidate['name']}
        {candidate['education']} | {candidate.get('current_role', 'Role TBD')}
        
        **Most Valuable:** {candidate['unique_strength']}
        """)
    
    with col3:
        if st.button("ğŸ‘ï¸ Details", key=f"details_{index}"):
            st.session_state[f"show_details_{index}"] = True
    
    # Show details if clicked
    if st.session_state.get(f"show_details_{index}", False):
        with st.expander("ğŸ“‹ Full Feedback", expanded=True):
            detail_view.render(candidate, match_score)
```

---

## ğŸš€ IMPLEMENTATION ROADMAP (24 hours)

### Hour 0-1: Setup & Config
```
- Create project structure
- Setup .env and config loader
- Initialize Git repo
- Create requirements.txt
```

### Hour 1-3: LLM Abstraction
```
- Implement LLMProvider abstract base
- Implement MistralProvider (with error handling)
- Test with sample prompts
- Create factory pattern
```

### Hour 3-8: Core Processors
```
- CompanyProfiler (config only, no LLM for now)
- CVParser (structure extraction)
- SkillExtractor (heuristic base + LLM integration)
- CultureAnalyzer (basic semantic matching)
- RedFlagDetector (comprehensive detection)
- Matcher (multi-layer scoring)
```

### Hour 8-11: Feedback Generation
```
- TemplateFeedback (base structure)
- FeedbackGenerator (hybrid mode)
- LLM integration for personalization
- Testing with sample data
```

### Hour 11-15: UI/Streamlit
```
- Main app structure (tabs)
- Company setup form
- CV upload widget
- Card rendering (Tinder-like)
- Detail view
- Beautiful CSS/styling
- Testing & polish
```

### Hour 15-18: Integration & Testing
```
- End-to-end flow testing
- Sample data preparation (diverse CVs)
- Error handling & edge cases
- Performance optimization
```

### Hour 18-22: Polish & Submission
```
- Record demo video (60 sec)
- Record tech video (60 sec)
- Create one-pager PDF
- Push to GitHub
- Create submission package
```

### Hour 22-24: Final Buffer
```
- Test all links
- Final review
- Submit with buffer
```

---

## ğŸ“ KEY FILES TO CREATE (in order)

1. `core/config.py` - Config loader
2. `core/llm_provider.py` - LLM abstraction
3. `processors/company_profiler.py` - Company setup
4. `processors/cv_parser.py` - Parse CVs
5. `processors/skill_extractor.py` - Extract skills (hybrid)
6. `processors/culture_analyzer.py` - Culture matching
7. `processors/red_flag_detector.py` - Comprehensive flags
8. `processors/matcher.py` - Multi-layer matching
9. `processors/feedback_generator.py` - Feedback (hybrid)
10. `ui/streamlit_app.py` - Main Streamlit app
11. `ui/components/card_view.py` - Beautiful cards
12. `ui/components/detail_view.py` - Feedback detail
13. `data/sample_cvs/` - Diverse sample data
14. `.env.example` - Config template
15. `requirements.txt` - Dependencies
16. `README.md` - Documentation

---

## âœ… SUCCESS CRITERIA FOR MVP

- âœ… LLM abstraction works (any provider pluggable)
- âœ… Process batch CVs (5+ diverse samples)
- âœ… Extract soft skills intelligently (hybrid)
- âœ… Detect comprehensive red flags
- âœ… Generate hybrid feedback (template + LLM magic)
- âœ… Beautiful Tinder-like card UI
- âœ… Detailed feedback view (full breakdown)
- âœ… End-to-end flow works without errors
- âœ… Ready for demo (compelling, clear)

---

## ğŸ¬ DEMO SCRIPT (60 seconds)

```
[0-5s]   "Hiring is broken. CVs drown in a pile."
         [Show: 100 CVs, overwhelmed recruiter]

[5-15s]  "What if we understood them deeply?"
         [Show: Inter-Sight logo, tagline]

[15-45s] Live Demo:
         - Upload 5 CVs
         - See Tinder cards (beautifully ranked)
         - Click one card â†’ Full feedback appears
         
         "Look at this: Not just 'you matched 60%'
          but 'here's why, here's your gaps, here's how to improve'"

[45-55s] "That's Inter-Sight. Intelligent feedback. For everyone."

[55-60s] "Ready for smarter hiring?"
```

---

## ğŸ¯ YOUR MISSION

Build this. Follow the roadmap. Use Mistral free tier. Create something beautiful.

**You got this.** ğŸ’ª

Let me know when you're ready to start. I'll give you the first file to create.
